{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient based optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- [Intro to machine learning](https://github.com/AI-Core/Strong-ML-Foundations/blob/master/Linear%20regression%2C%20loss%20functions%2C%20basic%20optimisation%20solutions.ipynb)\n",
    "\n",
    "[Previously](https://github.com/AI-Core/Strong-ML-Foundations/blob/master/Linear%20regression%2C%20loss%20functions%2C%20basic%20optimisation%20solutions.ipynb) we saw how we could optimise parameters of our models using grid search and random search. These two techniques had some fundamental flaws though...\n",
    "- we had to specify the range which we would search for parameter values within, and this might not contain the optimal parameter values (e.g. if we search all parameters from 1 to 10, but the ideal parameter value is 15, then we will never find an optimal parameterisation)\n",
    "- we don't use information about a current parameterisation as a heuristic for where to search next. We simply take either a totally random value (random search) or a predetermined value (grid search)\n",
    "- the time complexity of these optimisation techniques scales exponentially with the number of parameters. This is very bad (the neural networks which we will use soon can easily have millions of parameters).\n",
    "\n",
    "This notebook will walk through how we can use **gradient based optimisation** as another technique to find model parameterisations that perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a gradient?\n",
    "\n",
    "Let's imagine a function with an output $Y$ and an input $X$, with parameters $W$ and $b$ such that $Y=WX+b$. The **gradient of $Y$ with respect to $W$** is the amount that Y changes when we increase $W$ by 1 unit and hold all other values constant. The sign of the gradient (negative or positive) specifies whether $Y$ will increase (+) or decrease (-) as a result of this unit change in $W$. The magnitude of this gradient is the amount by which $Y$ changes. \n",
    "\n",
    "The gradient may also be referred to as the **derivative**. A *total* derivative of $Y$ with respect to $W$ is denoted as $\\frac{dY}{dW}$. This is the amount that $Y$ changes as we increase $W$, not necessarily holding variables in the function that are not $W$ constant.\n",
    "\n",
    "We denote the partial derivative of $Y$ with respect to $W$ as $\\frac{\\partial Y}{\\partial W}$ when we hold all variables except from $W$ constant.\n",
    "\n",
    "In this case, where $Y=WX+b$,\n",
    "\n",
    "$\\frac{\\partial Y}{\\partial W} = X$\n",
    "\n",
    "$\\frac{\\partial Y}{\\partial X} = W$\n",
    "\n",
    "$\\frac{\\partial Y}{\\partial b} = 1$\n",
    "\n",
    "Anyway, you should already be comfortable with gradients. For more info on gradients, see [here](https://mml-book.github.io/book/mml-book.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is gradient based optimisation all about?\n",
    "\n",
    "Grid search and random search evaluate the loss/error/cost (which tells us how bad our model is) for each different parameterisation that they test. \n",
    "\n",
    "Our loss is just a mathematical function that depends on the parameters of our model (for example, we used the mean squared error (MSE) loss function in the previous notebook).\n",
    "We would like to move our parameters to the point where loss is minimised.\n",
    "\n",
    "If we were to evaluate the value of our loss for every possible different parameterisation of our model, we would produce a **loss surface**. \n",
    "We would like to find the lowest point on this surface. \n",
    "At this point it will have a gradient (steepness) of zero with respect to the parameters.\n",
    "\n",
    "As our parameters move away from that minima in some direction, the gradient will increase in that direction.\n",
    "To get back to the minima, we should hence move our weights in the opposite direction.\n",
    "This tells us that wherever we are, we can decrease the current value of the loss by moving in the opposite \n",
    "direction to the gradient. This is at the core of gradient based optimisation.\n",
    "\n",
    "![](./images/grad-based-optim.jpg)\n",
    "\n",
    "Below is a more complex potential loss surface (vertical axis represents loss value, others represent parameter values).\n",
    "\n",
    "<img style=\"height: 200px\" src='./images/comp-loss-surface.png'/>\n",
    "\n",
    "**Note: because gradient based optimisation depends on us computing the gradient of the loss function, our loss function and model must be fully differentiable (they must be a smooth, continuous function).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by creating some fake data which we will try to fit using gradient based optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DON'T WORRY ABOUT THIS CELL, IT JUST SETS SOME STUFF UP THAT WAS CODED IN PREVIOUS NOTEBOOKS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_linear_data(m=20): \n",
    "    ground_truth_w = 2.3 # slope\n",
    "    ground_truth_b = -8 #intercept\n",
    "    X = np.random.randn(m)*2\n",
    "    Y = ground_truth_w*X + ground_truth_b + 0.2*np.random.randn(m)\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "    \n",
    "m = 50\n",
    "X, Y = sample_linear_data(m)\n",
    "plot_data(X, Y)\n",
    "\n",
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "    \n",
    "# DEFINE MEAN SQUARED ERROR LOSS FUNCTION\n",
    "def L(y_hat, labels):\n",
    "    errors = y_hat - labels # calculate errors\n",
    "    squared_errors = np.square(errors) # square errors\n",
    "    mean_squared_error = np.sum(squared_errors) / (m) # calculate mean \n",
    "    return mean_squared_error # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Gradient descent is an iterative, gradient based optimisation technique. As indicated above, we simply pass our data through our model and compute the gradient of the loss (which is a vector that points in the direction that our parameters should move to most increase the loss), then adjust the weights by a small amount in the opposite direction.\n",
    "\n",
    "### Gradient descent algorithm\n",
    "- randomly initialise model parameters\n",
    "- while not converged:\n",
    "    - Calculate the cost of this parameterisation (by evaluating how it performs on some examples) and the derivative of our cost with respect to (w.r.t) each parameter. This derivative points in the direction of steepest ascent (the gradient of the loss surface for this parameterisation). \n",
    "    - Update each parameter value by taking a step in the opposite direction. \n",
    "\n",
    "### The learning rate, $\\alpha$\n",
    "\n",
    "We will update our parameters by shifting them in the opposite direction to the gradient. But by what amount should we shift them in that direction?\n",
    "\n",
    "If the step size were some constant value, then our model might need to adjust its weights by some value smaller than this to reach a nearby minima. \n",
    "\n",
    "We know that the gradient at a minima is zero, and at this point we want our parameters to be moved with a step size of zero - so that they remain where they are, at the minima. So let's consider the weights of the model being updated by a step size proportional to the gradient. We call the proportionality constant the **learning rate**, and denote it as $\\alpha$.\n",
    "\n",
    "If the step size were directly equal to the gradient ($\\alpha=1$), gradient descent can fail to converge because the steps are too large (the same problem hence occurs if the learning rate is too large)\n",
    "\n",
    "![title](images/high-lr.jpg)\n",
    "\n",
    "So we include the learning rate to scale down the size of the steps. The learning rate should most likely be less than 1.\n",
    "\n",
    "If the learning rate is too low, then our model can take too long (too many gradient descent iterations) to converge\n",
    "\n",
    "![title](images/low-lr.jpg)\n",
    "\n",
    "You should play around with the learning rate and adjust it until your model converges.\n",
    "\n",
    "![title](images/convergence.jpg)\n",
    "\n",
    "The diagrams shown above visualise how a single weight affects the loss. A model with multiple weights would be optimised in the same way - we would just have more of these functions. We can think of each of the graphs as a cross section through a **loss surface**. A loss surface is shown below which visualises how the criterion of a model might vary with both parameters.\n",
    "\n",
    "<img style=\"height: 200px\" src='./images/comp-loss-surface.png'/>\n",
    "\n",
    "If we know the function that the loss is computed from and it is differentiable, then we can calculate the derivative of the loss with respect to our model parameters by hand.\n",
    "\n",
    "Below is a derivation for computing the rate of change (gradient) of the loss with respect to our model parameters when using a linear model and the mean squared error loss function.\n",
    "![title](images/NN1_single_grad_calc.jpg)\n",
    "\n",
    "Complete the class below to return the derivative of our loss w.r.t the weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.33806388907426 18.645804259675494\n"
     ]
    }
   ],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): \n",
    "        self.w = np.random.randn() # weight\n",
    "        self.b = np.random.randn() # bias\n",
    "    \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        y_hat = self.w*X + self.b\n",
    "        return y_hat\n",
    "    \n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b\n",
    "        \n",
    "    def calc_deriv(self, X, y_hat, labels):\n",
    "        diffs = y_hat - labels # calculate errors\n",
    "        dLdw = 2*np.array(np.sum(diffs*X) / m) # calculate derivative of loss with respect to weights\n",
    "        dLdb = 2*np.array(np.sum(diffs)/m) # calculate derivative of loss with respect to bias\n",
    "        return dLdw, dLdb\n",
    "    \n",
    "H = LinearHypothesis() # initialise our model\n",
    "y_hat = H(X) # make prediction\n",
    "dLdw, dLdb = H.calc_deriv(X, y_hat, Y) # calculate gradient of current loss with respect to model parameters\n",
    "print(dLdw, dLdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can complete the derivatives, complete the train function below to iteratively improve our parameter estimates to minimize the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost: 1.7652578622841164\n",
      "Weight values: 2.2590196027136136\n",
      "Bias values: -6.997999813037009\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "H = LinearHypothesis()\n",
    "\n",
    "def train(num_epochs, X, Y, H, L, plot_cost_curve=False):\n",
    "    all_costs = [] # initialise empty list of costs to plot later\n",
    "    for e in range(num_epochs): # for this many complete runs through the dataset\n",
    "        y_hat = H(X) # make predictions\n",
    "        cost = L(y_hat, Y) # compute loss \n",
    "        dLdw, dLdb = H.calc_deriv(X, y_hat, Y) # calculate gradient of current loss with respect to model parameters\n",
    "        new_w = H.w - learning_rate * dLdw # compute new model weight using gradient descent update rule\n",
    "        new_b = H.b - learning_rate * dLdb # compute new model bias using gradient descent update rule\n",
    "        H.update_params(new_w, new_b) # update model weight and bias\n",
    "        all_costs.append(cost) # add cost for this batch of examples to the list of costs (for plotting)\n",
    "    if plot_cost_curve: # plot stuff\n",
    "        plt.figure() # make a figure\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.plot(all_costs) # plot costs\n",
    "    print('Final cost:', cost)\n",
    "    print('Weight values:', H.w)\n",
    "    print('Bias values:', H.b)\n",
    "    #return cost, H.w\n",
    "    \n",
    "train(num_epochs, X, Y, H, L, plot_cost_curve=True) # train model and plot cost curve\n",
    "plot_h_vs_y(X, H(X), Y) # plot predictions and true data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why gradient based optimisation?\n",
    "We previously mentioned issues with grid search and random search:\n",
    "- our search region not containing an optimal parameterisation for our model\n",
    "- exponential increase in runtime with each additional parameter\n",
    "\n",
    "But beyond these, an advantage of using gradient based optimisation is that it follows a **heuristic** - an indication of how to improve. The heuristic is the gradient, which indicates what might be a good way to improve the weights. \n",
    "Grid and random search are not heuristic search methods. Each time they try a new parameterisation, they don't get any more information about where might be a good next parameterisation. Instead, they simply try a new set of values by choosing totally randomly (random search) or by picking a predetermined value at the next point on the grid (grid search).\n",
    "\n",
    "By using gradient descent, which follows a heuristic indication of where to try next (down the hill), our model can converge in much less iterations compared to grid or random search. If we firstly initialise our parameters near to the optima on the loss surface, gradient descent might only need a few updates, whereas grid or random searches will always take the same amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should we not pass the whole dataset through the model for each update?\n",
    "We know that to perform gradient based optimisation we need to pass inputs through the model (forward pass), and then compute the loss and find how it changes with respect to each of our model's parameters (backward pass). Modern datasets can be absolutely huge. This means that the forward pass can take a long time, as the function which our model represents has to be applied to each and every input given to it for a forward pass.\n",
    "\n",
    "Passing the full dataset through the model at each pass is called **full batch gradient descent**.\n",
    "\n",
    "### Why not just pass a single datapoint to the model for each update?\n",
    "We want our model to perform well on all examples, not just single examples. So we want to compute the loss and associated gradients over several examples to get an average gradient that should lead to better performance across any example, not just this specific one. If we only pass a single example through, the gradient won't be based on a representative sample.\n",
    "\n",
    "Passing single examples through the model at each pass is called **stochastic gradient descent**.\n",
    "\n",
    "## Mini-batch gradient descent\n",
    "The modern way to do training is neither full-batch (whole dataset) or fully stochastic (single datapoint). Instead we use mini-batch training, where we sample several (but not all) datapoints to compute a sample of the gradient, which we then use to update the model. Most optimisation algorithms converge much faster if they are allowed to rapidly compute approximate gradients rather than slowly compute exact gradients. The size of the mini-batch is called the **batch size**. Mini-batches are commonly incorrectly referred to as batches, but it's not that deep. \n",
    "\n",
    "We will experiment with the effect of batch size on the training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  0.45799791,  -2.3248527 ,   0.55923038,  -0.63919798],\n",
      "       [ -6.85061293, -13.34836063,  -7.0453984 ,  -8.91623928]]), array([[  0.99687095,  -1.82892223,  -1.21210407,   1.6438415 ],\n",
      "       [ -5.80035784, -12.35961331, -11.03832157,  -4.14365557]]), array([[ -0.58831206,  -1.23976505,  -1.20325613,   0.39818565],\n",
      "       [ -9.22116564, -10.85250806, -10.83710082,  -6.81819472]]), array([[ -0.61906694,   0.70571833,  -1.36013345,   0.48801835],\n",
      "       [ -9.12990259,  -6.49527346, -11.23051816,  -6.669073  ]]), array([[ 2.71835058,  1.15657447,  1.29515229,  0.12213295],\n",
      "       [-1.93279151, -5.69515654, -4.90552429, -7.72005603]]), array([[ 1.90022557,  1.88660385,  5.05793589,  2.29995235],\n",
      "       [-3.93363485, -3.65226603,  3.5109014 , -2.75630589]]), array([[  3.7096034 ,  -3.61904993,   2.22311047,  -1.03275958],\n",
      "       [  0.11629071, -16.31210454,  -2.97717698, -10.60732686]]), array([[  0.80117067,   0.91367047,   1.6091016 ,  -1.42362397],\n",
      "       [ -6.53861765,  -5.85837992,  -4.35545129, -11.37582958]]), array([[ -0.94933879,   1.07326214,   3.11502133,  -1.80666862],\n",
      "       [-10.04405444,  -5.48420534,  -0.9199981 , -12.11389694]]), array([[ -5.23091788,  -1.33123233,  -0.89781046,  -0.09178038],\n",
      "       [-20.14120473, -10.88630332, -10.32515237,  -8.26515395]]), array([[ -1.66863279,   4.6129591 ,   1.54084199,  -0.89932583],\n",
      "       [-11.83268954,   2.45132328,  -4.47703859, -10.01874544]]), array([[ -4.89016787,   1.30179264,   1.48457308,  -2.49536491],\n",
      "       [-19.3666318 ,  -5.32665248,  -4.63441053, -13.95852827]]), array([[  0.66097054,  -2.70244643],\n",
      "       [ -6.40832401, -14.42054191]])]\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from random import shuffle\n",
    "\n",
    "def create_batches(dataset, batch_size=4):\n",
    "    shuffle(dataset) # shuffle the dataset\n",
    "    idx = 0 # initialise starting point in dataset (index of first example to be put into the next batch)\n",
    "    batches = []\n",
    "    while idx < len(dataset): # while starting point index is less than the length of the dataset \n",
    "        if idx + batch_size < len(dataset): # if enough examples remain to make a whole batch\n",
    "            batch = dataset[idx: idx + batch_size] # make a batch from those examples \n",
    "        else: # otherwise\n",
    "            batch = dataset[idx:] # take however many examples remain (less than batch size)\n",
    "        batches.append(batch) # add this batch to the list of batches\n",
    "        idx += batch_size # increment the starting point for the next batch\n",
    "    batches = [np.array(list(zip(*b))) for b in batches] # unzip the batches into lists of inputs and outputs so batch = [all_inputs, all_outputs] rather than batch = [(input_1, output_1), ..., (input_batch_size, output_batch_size)]\n",
    "    return batches\n",
    "\n",
    "dataset = list(zip(X, Y))\n",
    "data_loader = create_batches(dataset, batch_size=4)\n",
    "print(data_loader)\n",
    "print(len(data_loader)) # should be m / batch_size rounded up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update our training function so that it performs mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full batch training\n",
      "Average inference (prediction) time: 0.011 milliseconds\n",
      "Average weight update time: 0.030 milliseconds\n",
      "Final cost: 0.04009431421221908\n",
      "\n",
      "Stochastic training\n",
      "Average inference (prediction) time: 0.003 milliseconds\n",
      "Average weight update time: 0.012 milliseconds\n",
      "Final cost: 4.725012852822647e-08\n",
      "\n",
      "Mini-batch training\n",
      "Average inference (prediction) time: 0.003 milliseconds\n",
      "Average weight update time: 0.012 milliseconds\n",
      "Final cost: 5.745928141965905e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "H = LinearHypothesis()\n",
    "m = 10000\n",
    "num_updates = 10 * m\n",
    "X, Y = sample_linear_data(m)\n",
    "dataset = list(zip(X, Y))\n",
    "\n",
    "def train(num_updates, data_loader, H, L, plot_cost_curve=False, plot_h=False):\n",
    "    costs = [] # initialise empty list of costs to plot later\n",
    "    update_idx = 0\n",
    "    inference_times = []\n",
    "    update_times = []\n",
    "    while update_idx < num_updates: # for this many complete runs through the dataset\n",
    "        batch_costs = []\n",
    "        for x, y in data_loader:\n",
    "            inference_start = time() # get time at start of inference\n",
    "            y_hat = H(x) # make predictions\n",
    "            inference_times.append(time() - inference_start) # add duration of inference\n",
    "            cost = L(y_hat, y) # compute loss \n",
    "            update_start = time()\n",
    "            dLdw, dLdb = H.calc_deriv(x, y_hat, y) # calculate gradient of current loss with respect to model parameters\n",
    "            new_w = H.w - learning_rate * dLdw # compute new model weight using gradient descent update rule\n",
    "            new_b = H.b - learning_rate * dLdb # compute new model bias using gradient descent update rule\n",
    "            H.update_params(new_w, new_b) # update model weight and bias\n",
    "            update_times.append(time() - update_start)\n",
    "            update_idx += 1\n",
    "            batch_costs.append(cost)\n",
    "            #prop_complete = round((update_idx / num_updates) * 100)     \n",
    "            #print('\\r' + [\"|\", \"/\", \"-\", \"\\\\\"][update_idx % 4], end='')\n",
    "            #print(f'\\r[{prop_complete * \"=\" + (0 - prop_complete) * \"-\"}]', end='')\n",
    "        costs.append(np.mean(batch_costs)) # add cost for this batch of examples to the list of costs (for plotting)\n",
    "    if plot_cost_curve: # plot stuff\n",
    "        plt.figure() # make a figure\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Update idx')\n",
    "        plt.plot(costs) # plot costs\n",
    "        plt.show()\n",
    "    if plot_h:\n",
    "        plot_h_vs_y(X, H(X), Y)\n",
    "    print(f'Average inference (prediction) time: {np.mean(inference_times)*1000:.3f} milliseconds')\n",
    "    print(f'Average weight update time: {np.mean(update_times)*1000:.3f} milliseconds')\n",
    "    print('Final cost:', cost)\n",
    "#     print('Weight values:', H.w)\n",
    "#     print('Bias values:', H.b)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print('Full batch training')\n",
    "full_batch_data_loader = create_batches(dataset, batch_size=len(dataset))\n",
    "train(num_updates, full_batch_data_loader, H, L, plot_cost_curve=False)\n",
    "\n",
    "print('Stochastic training')\n",
    "stochastic_data_loader = create_batches(dataset, batch_size=1)\n",
    "train(num_updates, stochastic_data_loader, H, L, plot_cost_curve=False)\n",
    "\n",
    "print('Mini-batch training')\n",
    "mini_batch_data_loader = create_batches(dataset, batch_size=32)\n",
    "train(num_updates, mini_batch_data_loader, H, L, plot_cost_curve=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that stochastic and mini-batch training perform inference (making predictions) >3x faster and make updates 2.5x faster than full batch training. For larger datasets or inputs, these differences would be exaggerated even further. This is because the model has to pass the whole dataset forward to make a prediction and then compute the average gradient from every single one of those values to compute the weight and bias updates.\n",
    "\n",
    "For a larger dataset with more complex example features and/or labels, stochastic gradient descent may not converge, because single examples may rarely produce a gradient that is representative of an update that would reduce the error for all examples, rather than just this example.\n",
    "\n",
    "## You made it!\n",
    "In this notebook we covered what gradient based optimisation is and why it can be advantageous compared to grid or random search.\n",
    "We also learnt about why it can be useful to load our dataset to our model in mini-batches, and implemented this in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps \n",
    "- [Multivariate regression and feature normalisation]()\n",
    "- [Bias, variance and generalisation]()\n",
    "- [More advanced gradient based optimisation techniques]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
